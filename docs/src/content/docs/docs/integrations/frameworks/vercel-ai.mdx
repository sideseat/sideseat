---
title: Vercel AI SDK
description: Integrate SideSeat with the Vercel AI SDK for TypeScript AI app visibility.
---

import { Aside, Steps, Tabs, TabItem } from '@astrojs/starlight/components';

[Vercel AI SDK](https://sdk.vercel.ai/) is a TypeScript toolkit for building AI applications. SideSeat captures traces from `generateText`, `streamText`, and other AI SDK calls via OpenTelemetry.

<Aside type="note">
The Vercel AI SDK is TypeScript-only. For Python frameworks, see [Strands](/docs/integrations/frameworks/strands/) or [LangGraph](/docs/integrations/frameworks/langgraph/).
</Aside>

## Quick Start

<Steps>
1. **Install dependencies**

   ```bash
   npm install @sideseat/sdk ai @ai-sdk/openai
   ```

2. **Add telemetry**

   ```typescript
   import { init } from '@sideseat/sdk';
   init();

   import { generateText } from 'ai';
   import { openai } from '@ai-sdk/openai';

   const { text } = await generateText({
     model: openai('gpt-5-mini'),
     prompt: 'What is the capital of France?',
     experimental_telemetry: { isEnabled: true },
   });

   console.log(text);
   ```

3. **View runs**

   Open [http://localhost:5388](http://localhost:5388) to see your runs.
</Steps>

<Aside type="caution">
`experimental_telemetry: { isEnabled: true }` is required on each `generateText` or `streamText` call. Without it, no traces are emitted.
</Aside>

## Without SideSeat SDK

Use the OpenTelemetry SDK directly:

```typescript
import { NodeSDK } from '@opentelemetry/sdk-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';

const sdk = new NodeSDK({
  traceExporter: new OTLPTraceExporter({
    url: 'http://localhost:5388/otel/default/v1/traces',
  }),
});

sdk.start();
```

Or set the environment variable:

```bash
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:5388/otel/default/v1/traces
```

## What You'll See

SideSeat shows each `generateText`/`streamText` call with the model name, token counts, prompt messages, and the full response. Tool calls appear as child spans.

## Streaming

```typescript
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = streamText({
  model: openai('gpt-5-mini'),
  prompt: 'Tell me a story',
  experimental_telemetry: { isEnabled: true },
});

for await (const chunk of result.textStream) {
  process.stdout.write(chunk);
}
```

## Tool Calls

```typescript
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { text } = await generateText({
  model: openai('gpt-5-mini'),
  prompt: 'What is the weather in Paris?',
  tools: {
    getWeather: tool({
      description: 'Get weather for a location',
      parameters: z.object({ location: z.string() }),
      execute: async ({ location }) => `Sunny in ${location}`,
    }),
  },
  experimental_telemetry: { isEnabled: true },
});
```

## Next Steps

- [JavaScript SDK](/docs/sdks/javascript/) — SDK reference
- [Core Concepts](/docs/concepts/) — understanding runs, steps, and messages
