---
title: First Run
description: Run SideSeat locally and see your first agent run in minutes.
---

import { Tabs, TabItem, Aside, Steps } from '@astrojs/starlight/components';

Get from zero to your first run in the local AI development workbench.

## Prerequisites

- Node.js 18+ (to run SideSeat)
- Python 3.9+ or Node.js 18+ (for your AI app)
- Model credentials for your provider (if required)

## Start SideSeat

<Steps>
1. **Run SideSeat locally**

   ```bash
   npx sideseat
   ```

   You'll see output like:

   ```
   SideSeat v1.x
     Local: http://127.0.0.1:5388
     OTLP:  http://127.0.0.1:5388/otel/default/v1/traces
   ```

2. **Open the workbench**

   Navigate to [http://localhost:5388](http://localhost:5388) in your browser.
</Steps>

## Install the SDK

<Tabs>
  <TabItem label="Python">
    ```bash
    pip install sideseat
    # or
    uv add sideseat
    ```
  </TabItem>
  <TabItem label="JavaScript">
    ```bash
    npm install @sideseat/sdk
    ```
  </TabItem>
</Tabs>

## Instrument Your App

Add two lines at the top of your entry point:

<Tabs>
  <TabItem label="Python (Strands)">
    ```python
    from sideseat import SideSeat, Frameworks
    from strands import Agent

    SideSeat(framework=Frameworks.Strands)

    agent = Agent()
    response = agent("What is the capital of France?")
    print(response)
    ```
  </TabItem>
  <TabItem label="Python (Bedrock)">
    ```python
    from sideseat import SideSeat, Frameworks
    import boto3

    SideSeat(framework=Frameworks.Bedrock)

    bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")
    response = bedrock.converse(
        modelId="us.anthropic.claude-sonnet-4-5-20250929-v1:0",
        messages=[{"role": "user", "content": [{"text": "What is the capital of France?"}]}],
    )

    print(response["output"]["message"]["content"][0]["text"])
    ```
  </TabItem>
  <TabItem label="JavaScript (Vercel AI)">
    ```typescript
    import { generateText } from 'ai';
    import { bedrock } from '@ai-sdk/amazon-bedrock';
    import { init } from '@sideseat/sdk';

    init();

    const { text } = await generateText({
      model: bedrock('anthropic.claude-sonnet-4-5-20250929-v1:0'),
      prompt: 'What is the capital of France?',
      experimental_telemetry: { isEnabled: true },
    });
    console.log(text);
    ```
  </TabItem>
</Tabs>

The SDK auto-detects your framework and configures telemetry accordingly.

## What You'll See

Run your app. A new run appears in the workbench showing a timeline of each LLM call, tool execution, and response. Token counts and costs are calculated automatically.

You should see:
- A live run timeline with each step
- Prompt and response messages grouped by step
- Token, latency, and cost summaries

## Why Local?

SideSeat runs locally by default. Your data stays on your machine.

| Benefit | What It Means |
|---------|---------------|
| **No signup** | Run `npx sideseat` and start debugging immediately |
| **No data egress** | Traces stay on your machine — no cloud uploads |
| **No latency** | Real-time streaming without network roundtrips |
| **No vendor lock-in** | Standard OpenTelemetry traces work with any backend |

<Aside type="note">
If you use cloud LLMs, those requests still go to your provider. SideSeat only stores what your app sends via OpenTelemetry.
</Aside>

## Next Steps

- [Core Concepts](/docs/concepts/) — understand runs, steps, and messages
- [Integrations](/docs/integrations/) — connect your framework or provider
- [Troubleshooting](/docs/troubleshooting/) — fix common issues
